{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nsmc-huggingface-koelectra.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdjLL9gvyVCR"
      },
      "source": [
        "참고한 블로그 : https://heegyukim.medium.com/huggingface-koelectra%EB%A1%9C-nsmc-%EA%B0%90%EC%84%B1%EB%B6%84%EB%A5%98%EB%AA%A8%EB%8D%B8%ED%95%99%EC%8A%B5%ED%95%98%EA%B8%B0-1a23a0c704af\n",
        "\n",
        "# Pytorch + HuggingFace \n",
        "## KoElectra Model\n",
        "박장원님의 KoElectra-small 사용<br>\n",
        "https://monologg.kr/2020/05/02/koelectra-part1/<br>\n",
        "https://github.com/monologg/KoELECTRA\n",
        "\n",
        "## Dataset\n",
        "네이버 영화 리뷰 데이터셋<br>\n",
        "https://github.com/e9t/nsmc\n",
        "\n",
        "## References\n",
        "- https://huggingface.co/transformers/training.html\n",
        "- https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html\n",
        "- https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html\n",
        "- https://wikidocs.net/44249\n",
        "\n",
        "## 주의사항\n",
        "꼭 GPU로 해주세요 - 1epoch 당 약 20분 소요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc7P9wzHv0LE"
      },
      "source": [
        "# HuggingFace transformers 설치 및 NSMC 데이터셋 다운로드\n",
        "!pip install transformers\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q33PkINy4Q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7dc3c25-7100-4736-e10b-37920f7e3619"
      },
      "source": [
        "!head ratings_train.txt\n",
        "!head ratings_test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id\tdocument\tlabel\n",
            "9976970\t아 더빙.. 진짜 짜증나네요 목소리\t0\n",
            "3819312\t흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\t1\n",
            "10265843\t너무재밓었다그래서보는것을추천한다\t0\n",
            "9045019\t교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\t0\n",
            "6483659\t사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\t1\n",
            "5403919\t막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.\t0\n",
            "7797314\t원작의 긴장감을 제대로 살려내지못했다.\t0\n",
            "9443947\t별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네\t0\n",
            "7156791\t액션이 없는데도 재미 있는 몇안되는 영화\t1\n",
            "id\tdocument\tlabel\n",
            "6270596\t굳 ㅋ\t1\n",
            "9274899\tGDNTOPCLASSINTHECLUB\t0\n",
            "8544678\t뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\t0\n",
            "6825595\t지루하지는 않은데 완전 막장임... 돈주고 보기에는....\t0\n",
            "6723715\t3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??\t0\n",
            "7898805\t음악이 주가 된, 최고의 음악영화\t1\n",
            "6315043\t진정한 쓰레기\t0\n",
            "6097171\t마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다\t0\n",
            "8932678\t갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한cg남무 아 그립다 동사서독같은 영화가 이건 3류아류작이다\t0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iFVHYMAwQUt"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, ElectraForSequenceClassification, AdamW\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i7pg7DaGsxp"
      },
      "source": [
        "# GPU 사용\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq91g3bGwfeV"
      },
      "source": [
        "# Dataset 만들어서 불러오기 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZKIQNjZwdn1"
      },
      "source": [
        "class NSMCDataset(Dataset):\n",
        "  \n",
        "  def __init__(self, csv_file):\n",
        "    # 일부 값중에 NaN이 있음...\n",
        "    self.dataset = pd.read_csv(csv_file, sep='\\t').dropna(axis=0) \n",
        "    # 중복제거\n",
        "    self.dataset.drop_duplicates(subset=['document'], inplace=True)\n",
        "    #ko-electra discriminator 사용\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-small-v2-discriminator\")\n",
        "\n",
        "    print(self.dataset.describe())\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    row = self.dataset.iloc[idx, 1:3].values\n",
        "    text = row[0]\n",
        "    y = row[1]\n",
        "\n",
        "    inputs = self.tokenizer(\n",
        "        text, \n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        pad_to_max_length=True,\n",
        "        add_special_tokens=True\n",
        "        )\n",
        "    \n",
        "    input_ids = inputs['input_ids'][0]\n",
        "    attention_mask = inputs['attention_mask'][0]\n",
        "\n",
        "    return input_ids, attention_mask, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESSNkTcXwfUe"
      },
      "source": [
        "train_dataset = NSMCDataset(\"ratings_train.txt\")\n",
        "test_dataset = NSMCDataset(\"ratings_test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJiAJPUDz40W"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7-jRPQXz2r5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db2b106-947f-47d5-e95b-1cc219c681d1"
      },
      "source": [
        "model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-small-v2-discriminator\").to(device)\n",
        "\n",
        "# 한번 실행해보기\n",
        "text, attention_mask, y = train_dataset[0]\n",
        "model(text.unsqueeze(0).to(device), attention_mask=attention_mask.unsqueeze(0).to(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at monologg/koelectra-small-v2-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v2-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput([('logits',\n",
              "                           tensor([[0.0170, 0.0433]], device='cuda:0', grad_fn=<AddmmBackward>))])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edb0aIFaXr4D"
      },
      "source": [
        "#model.load_state_dict(torch.load(\"model.pt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp6x4GHtz46u"
      },
      "source": [
        "# 모델 레이어 보기\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmou0LFl0R_X"
      },
      "source": [
        "# Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NpXwESN0Q4h"
      },
      "source": [
        "epochs = 3\n",
        "batch_size = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPzxoo4H274J"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-BRNeE226HH"
      },
      "source": [
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  total_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  batches = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    y_batch = y_batch.to(device)\n",
        "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
        "    loss = F.cross_entropy(y_pred, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "    correct += (predicted == y_batch).sum()\n",
        "    total += len(y_batch)\n",
        "\n",
        "    batches += 1\n",
        "    if batches % 100 == 0:\n",
        "      print(\"Batch Loss:\", total_loss, \"Accuracy:\", correct.float() / total)\n",
        "  \n",
        "  losses.append(total_loss)\n",
        "  accuracies.append(correct.float() / total)\n",
        "  print(\"Train Loss:\", total_loss, \"Accuracy:\", correct.float() / total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQK4R6n4JgVU"
      },
      "source": [
        "losses, accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvEB8g7IFbsD"
      },
      "source": [
        "테스트 데이터셋 정확도 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QiALUqm4juf"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
        "  y_batch = y_batch.to(device)\n",
        "  y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
        "  _, predicted = torch.max(y_pred, 1)\n",
        "  test_correct += (predicted == y_batch).sum()\n",
        "  test_total += len(y_batch)\n",
        "\n",
        "print(\"Accuracy:\", test_correct.float() / test_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrcPWEa5U8JZ"
      },
      "source": [
        "# 모델 저장하기\n",
        "torch.save(model.state_dict(), \"model.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}